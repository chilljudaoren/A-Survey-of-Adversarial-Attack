<h1 align="center">ğŸ¤—ğŸ¤— Awesome VLMs Safety ğŸ¤—ğŸ¤—</h1>

<p align="center">
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg" alt="Awesome Badge"></a>
    <img src="https://badges.toozhao.com/badges/01JM4JCV43N3ARA3BC25QSBH0S/blue.svg" alt="Custom Badge" />
    <a href="https://creativecommons.org/licenses/by-nc/4.0/"><img src="https://img.shields.io/badge/License-CC_BY--NC_4.0-lightgrey.svg" alt="License Badge"></a>
    <a href="https://github.com/XuankunRong/Awesome-LVLM-Safety"><img src="https://img.shields.io/github/stars/XuankunRong/Awesome-LVLM-Safety?style=social" alt="GitHub stars"></a>
</p>

This repository is currently under construction.

![awesome](https://github.com/chilljudaoren/A-Survey-of-Adversarial-Attack/blob/main/images/overview.png)
This is the repository of <b><q>Adversarial Attacks on Vision-Language Multimodal Systems: A Survey</q></b>, a systematic survey of VLM adversarial attack studies in various adversarial attacks including Adversarial Jailbreak Attack, Adversarail Prompt Attack, Knowledge-based Adversarial Attack, etc. For details, please refer to:

<h2> ğŸ™Œ Abstract </h2>

With the boom of artificial intelligence, multimodal deep learning, especially vision-language multimodal systems (VLMS), has advanced remarkably and found wide applications. However, their adversarial security issues hamper further development. This paper comprehensively reviews adversarial attacks on VLMS. It first details multimodal deep-learning key technologies and the features of relevant models. Then, it deeply analyzes the adversarial vulnerabilities of VLMS. Next, it systematically reviews various adversarial attack methods in multimodal systems, covering task-specific attacks and attacks on different models, with a detailed analysis of their principles, implementations, and effects. Finally, it looks ahead to the future of multimodal adversarial attacks, suggesting research directions like improving attack universality and black-box attack capabilities. This review offers technical references, promotes research on multimodal adversarial attacks, and helps enhance VLMS security for reliable real-world applications.    

<h2> ğŸ“œ Table of Contents </h2>

<h2 id="News"> ğŸ”¥ News </h2>

â€¼ï¸ **We plan to update the arXiv version of our survey paper soon. If your paper is missing from this repository, feel free to contact us or open an issue!**

<h2 id="awesome-papers"> ğŸ‘€ Awesome Papers </h2>

<h3 id="VLP"> ğŸ‘€ Adversarial Attack on VLP </h3>

* [ACM MM 2022] Towards Adversarial Attack on Vision-Language Pre-training
 Models [[Paper](https://arxiv.org/pdf/2206.09391)][[Code](https://github.com/adversarial-for-goodness/Co-Attack)]
